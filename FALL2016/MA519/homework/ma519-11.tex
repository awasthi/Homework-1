\begin{problem}[DasGupta 7.2 (a), (b), (c), (d), (e)]
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Suppose \(E|X_n-c|^\alpha\to 0\), where \(0<\alpha<1\). Does
    \(X_n\) necessarily converge in probability to \(c\)?
  \item Suppose \(a_n(X_n-\theta)\yright{\calL}N(0,1)\). Under what
    condition on \(a_n\) can we conclude that \(X_n\yright{\calP}\theta\)?
  \item \(\rmo_p(1)+\rmO_p(1)=?\)
  \item \(\rmo_p(1)\rmO_p(1)=?\)
  \item \(\rmo_p(1)+\rmo_p(1)\rmO_p(1)=?\)
  \end{enumerate}
\end{problem}
\begin{solution}
  For part (a) we show that indeed \(E(|X_n-c|^\alpha)\to 0\) implies
  \(X_n\yright{\calP}c\). Let \(\varepsilon>0\) be given. By Markov's
  inequality, we have
  \[
    P(|X_n-c|>\varepsilon)=
    P(|X_n-c|^\alpha>\varepsilon^\alpha)
    \leq\frac{E(|X_n-c|^\alpha)}{\varepsilon^\alpha}.
  \]
  Since \(E(|X_n-c|^\alpha)\to 0\) as \(n\to\infty\), it follows that
  \(P(|X_n-c|>\varepsilon)\) as \(n\to\infty\); i.e., \(X_n\) converges to
  \(c\) in probability.

  For part (b), suppose \(a_n(X_n-\theta)\yright{\calL}N(0,1)\). Under the
  hypotheses of the central limit theorem (i.e., \(Y_n\) are i.i.d.\@
  random variables with \(E(Y_n)=\theta\), \(\Var(Y_n)=1\), and setting
  \(X_n\defeq\frac{1}{n}\sum_{k=1}^n Y_n\) and \(a_n\defeq\sqrt{n}\)) we
  can say that \(a_n(X_n-\theta)\yright{\calL}N(0,1)\) implies
  \(X_n\to\theta\). More generally, we would need

  For part (c), suppose \(\{a_n\}\) and \(\{b_n\}\) are sequences such that
  \(a_n=\rmo_p(1)\) and \(b_n=\rmO_p(1)\), then for the sequence
  \(\{c_n\defeq a_n+b_n\}\) the most we can expect is
  \(c_n=\rmO_p(1)\). Indeed, we know that if a sequence is \(\rmo_p(1)\)
  then it is also \(\rmO_p(1)\) therefore there exists \(K_1\) and \(K_2\)
  such that \(|a_n|\leq K_1\), \(|b_n|\leq K_2\) for all \(n\geq
  1\). Therefore, \(|c_n|\leq K_1+K_2\) for all \(n\geq 1\).
\end{solution}
\newpage

\begin{problem}[DasGupta 7.3 {\protect[Monte Carlo]}]
  Consider the purely mathematical problem of finding a definite integral
  \(f(x)\diff x\) for some (possibly complicated) function \(f(x)\). Show
  that the SLLN provides a method for approximately finding the value of
  the integral by using appropriate averages
  \(\frac{1}{n}\sum_{k=1}^n f(X_k)\).

  Numerical analysts call this Monte Carlo integration.
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.4 (a), (b)]
  Suppose \(X_1,\dotsc,\) are i.i.d.\@ and that \(E(X_1)=\mu\neq 0\),
  \(\Var(X_1)=\sigma^2<\infty\). Let \(S_{m,p}=\sum_{k=1}^m X_k^p\),
  \(m\geq 1\), \(p=1,2\).
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Identify with proof the almost sure limit of \(S_{m,1}/S_{n,1}\)
    for fixed \(m\), and \(n\to\infty\).
  \item Identify with proof the almost sure limit of \(S_{n-m,1}/S_{n,1}\)
    for fixed \(m\), and \(n\to\infty\).
  \end{enumerate}
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.5 (a)]
  Let \(A_n\), \(n\geq 1\), \(A\) be events with respect to a common sample
  space \(\Omega\).
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Prove that \(I_{A_n}\yright{\calL} I_A\) if and only if \(P(A_n)\to
    P(A)\).
  \end{enumerate}
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.11 {\protect[Sample Maximum]}]
  Let \(X_k\), \(k\geq 1\), be an i.i.d.\@ sequence, and \(X_{(n)}\) the
  maximum of \(X_1,\dotsc,X_n\). Let \(\xi(F)=\sup\{\,x:F(x)<1\,\}\), where
  \(F\) is the common CDF of the \(X_k\). Prove that
  \(X_{(n)}\yright{\text{a.s.}}\xi(F)\).
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.14 (a)]
  Suppose \(X_k\) are i.i.d.\@ standard Cauchy. Show that
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item \(P(\text{\(|X_n|>n\) infinitely often})=1\).
  \end{enumerate}
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.16 {\protect[Coupon Collection]}]
  Cereal boxes contain independently and with equal probability exactly one
  of \(n\) different celebrity pictures. Someone having the entire set of
  \(n\) pictures can cash them in for money. Let \(W_n\) be the minimum
  number of cereal boxes one would need to purchase to own a complete set
  of the pictures. Find a sequence \(a_n\) such that
  \(W_n/a_n\yright{\calP} 1\).

  \noindent (\emph{Hint:} Approximate the mean of \(W_n\).)
\end{problem}
\begin{solution}
  Let \(X_n\sim\Geom(\frac{n-k}{n})\)
\end{solution}
\newpage

\begin{problem}[DasGupta 7.17]
  Let \(X\sim\Bin(n,p)\). Show that \((X_n/n)^2\) and
  \(X_n(X_n-1)/(n(n-1))\) both converging in probability to \(p^2\). Do
  they converge almost surely?
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.21]
  Let \(X_1,X_2,\dotsc,\) be i.i.d.\@ \(U[0,1]\). Let
  \[
    G_n=(X_1\dotsm X_n)^{1/n}.
  \]
  Find \(c\) such that \(G_n\yright{\calP} c\).
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.30 {\protect[Conceptual]}]
  Suppose \(X_n\yright{\calL}X\), and also \(Y_n\yright{\calL} X\). Does
  this mean that \(X_n-Y_n\) converge in distribution to (the point mass
  at) zero?
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[DasGupta 7.31 (a)]
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Suppose \(a_n(X_n-\theta)\to N(0,\tau^2)\); what can be said
    about the limiting distribution of \(|X_n|\), when \(\theta\neq 0\),
    \(\theta=0\)?
  \end{enumerate}
\end{problem}
\begin{solution}

\end{solution}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../MA519-HW-Current"
%%% End:
