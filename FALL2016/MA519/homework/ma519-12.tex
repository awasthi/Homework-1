\begin{problem}[Handout 15, \# 10]
  Consider the experiment of picking one word at random from the sentence
  \begin{quote}
    \textsl{All is well in the newell family}
  \end{quote}
  Let \(X\) be the length of the word selected and \(Y\) the number of Ls
  in it. Find in a tabular form the joint PMF of \((X,Y)\), their marginal
  PMFs, means, and variances, and the correlation between \(X\) and \(Y\).
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[Handout 15, \# 11]
  Consider the joint PMF \(p(x,y)=cxy\), \(1\leq x\leq 3\), \(1\leq y\leq
  3\).
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Find the normalizing constant \(c\).
  \item Are \(X\) and \(Y\) independent? Prove your claim.
  \item Find the expectations of \(X\), \(Y\), and \(XY\).
  \end{enumerate}
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[Handout 15, \# 12]
  A fair die is rolled twice. Let \(X\) be the maximum and \(Y\) the
  minimum of the two rolls. By using the joint PMF of \(X\) and \(Y\)
  worked out in the text, find the PMF of \(\frac{X}{Y}\), and hence the
  mean of \(\frac{X}{Y}\).
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[Handout 15, \# 13]
  Two random variables have the joint PMF \(p(x,x+1)=\frac{1}{n+1}\),
  \(x=0,\dotsc,n\). Answer the following question with as little
  calculation as possible.
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Are \(X\) and \(Y\) independent?
  \item What is the variance of \(Y-X\)?
  \item What is \(\Var(Y\,|\,{X=1})\)?
  \end{enumerate}
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[Handout 15, \# 14]
  \emph{(Binomial Conditional Distribution).} Suppose \(X\) and \(Y\) are
  independent random variables, and \(X\sim\Bin(m,p)\),
  \(Y\sim\Bin(n,p)\). Show that the conditional distribution of \(X\) given
  by \(X+Y=t\) is a hypergeometric distribution; identify the parameters of
  this hypergeometric distribution.
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[Handout 15, \# 15]
  Suppose a fair die is rolled twice. Let \(X\) and \(Y\) be the two
  rolls. Find the following with as little calculation as possible.
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item \(E(X+Y\,|\,Y=y)\).
  \item \(E(XY\,|\,Y=y)\).
  \item \(\Var(X^2Y\,|\,Y=y)\).
  \item \(\rho_{X+Y,X-Y}\).
  \end{enumerate}
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[Handout 15, \# 16]
  \emph{(A Standard Deviation Inequality).} Let \(X\) and \(Y\) be two
  random variables. Show that \(\sigma_{X+Y}\leq\sigma_X+\sigma_Y\).
\end{problem}
\begin{solution}
  Suppose \(\sigma_X\) and \(\sigma_Y\) exist and are finite. We want to
  show
  \[
    \sigma_{X+Y}\leq\sigma_X+\sigma_Y;
  \]
  this is the same as showing that
  \begin{align*}
    \sigma_{X+Y}^2
    &\leq\sigma_X+\sigma_Y^2+2\sigma_X\sigma_Y\\
    \Var(X+Y)&\leq\Var(X)+\Var(Y)+2[\Var(X)\Var(Y)]^{\frac{1}{2}}.
  \end{align*}

  First, let us expand \(\Var(X+Y)\) using the definition of variance, we
  have
  \begin{align*}
    \Var(X+Y)
    &=E\bigl((X+Y)^2\bigr)-E(X+Y)^2\\
    &=E(X^2)+2E(XY)+E(Y^2)-E(X)^2-2E(X)E(Y)-E(Y)^2\\
    &=\bigl(E(X^2)-E(X)^2\bigr)+\bigl(E(Y^2)-E(Y)^2\bigr)+2[E(XY)-E(X)E(Y)]\\
    &=\Var(X)+\Var(Y)+2[E(XY)-E(X)E(Y)].
  \end{align*}
  Therefore, it suffices to show that
  \[
    E(XY)-E(X)E(Y)\leq [\Var(X)\Var(Y)]^{\frac{1}{2}}.
  \]

  By the Cauchy--Schwartz inequality, we have
  \begin{align*}
    E(XY)-E(X)E(Y)
    &\leq
      {[E(X^2)E(Y^2)]}^{\frac{1}{2}}-{[E(X)^2E(Y)^2]}^{\frac{1}{2}}\\
    &=
  \end{align*}
\end{solution}
\newpage

\begin{problem}[Handout 15, \# 17]
  Seven balls are distributed randomly in seven cells. Let \(X_k\) be
  the number of cells containing exactly \(k\) balls. Using the
  probabilities tabulated in II, 5, write down the joint distribution of
  \(X_2,X_3\).
\end{problem}
\begin{solution}
  The tabled referenced in this problem is on p.\@ 40 of Feller. Let us
  write down a table of our own for the joint distribution of
  \((X_2,X_3)\):
  \[
    \begin{tabular}{c|c|c|c|c|}
      \(X_3\backslash X_2\)&0&1&2&3\\\hline
      0&\num{0.047539}&\num{0.156368}&\num{0.321295}&\num{0.107098}\\\hline
      1&\num{0.108883}&\num{0.214197}&\num{0.026775}&0\\\hline
      2&\num{0.017850}&0&0&0\\\hline
    \end{tabular}
  \]

  Let us do a sanity check by summing over all of the entries in the table
  above
  \[
    \num{0.047539}+\num{0.156368}+\num{0.321295}+\num{0.107098}+
    \num{0.108883}+\num{0.214197}+\num{0.026775}+0+\num{0.017850}+0+0+0\approx
    1.\qedhere
  \]
\end{solution}
\newpage

\begin{problem}[Handout 15, \# 18]
  Two ideal dice are thrown. Let \(X\) be the score on the first die and
  \(Y\) be the larger of two scores.
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Write down the joint distribution of \(X\) and \(Y\).
  \item Find the means, the variances, and the covariance.
  \end{enumerate}
  \end{problem}
\begin{solution}
  For part (a): The random variable \(X\) takes on integer values between
  zero and six and so does \(Y\). Moreover, the dependence of \(Y\) on
  \(X\) tells us that \(P(\{\,X=k\,\}\cap\{\,Y=\ell\,\})=0\) if
  \(\ell<k\); this allows us to fill in a significant portion of the joint
  distribution table:
  \[
    \begin{tabular}{c|c|c|c|c|c|c|c|}
      \(Y\bigsetminus X\)&0&1&2&3&&&\\\hline
      0\\\hline
      1\\\hline
      2\\\hline
      3\\\hline
      4\\\hline
      5\\\hline
      6\\\hline
    \end{tabular}
  \]
\end{solution}
\newpage

\begin{problem}[Handout 15, \# 19]
  Let \(X_1\) and \(X_2\) be independent and have the common
  geometric distribution \(\{q^kp\}\) (as in problem 4). Show without
  calculations that the \emph{conditional distribution of \(X_1\) given
    \(X_1+X_2\)is uniform}, that is,
  \begin{equation}
    \label{eq:12:uniform-conditional-pmf}
    P(X_1=k\,|\,X_1+X_2=n)=\frac{1}{n+1},\quad k=0,\dotsc,n.
  \end{equation}
\end{problem}
\begin{solution}

\end{solution}
\newpage

\begin{problem}[Handout 15, \# 20]
  If two random variables \(X\) and \(Y\) assume only two values
  each, and if \(\Cov(X,Y)=0\), then \(X\) and \(Y\) are
  independent.
\end{problem}
\begin{solution}

\end{solution}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../MA519-HW-Current"
%%% End:
