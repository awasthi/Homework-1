\section{Homework 12}
\begin{problem}[Handout 15, \# 10]
  Consider the experiment of picking one word at random from the sentence
  \begin{quote}
    \textsl{All is well in the newell family}
  \end{quote}
  Let \(X\) be the length of the word selected and \(Y\) the number of Ls
  in it. Find in a tabular form the joint PMF of \((X,Y)\), their marginal
  PMFs, means, and variances, and the correlation between \(X\) and \(Y\).
\end{problem}
\begin{solution}
  The joint PMF of $(X,Y)$ is given by
  \begin{center}
    \begin{tabular}{L|L|L|L|L|L|}
      _Y\backslash^X&2&3&4&5&6\\\hline
      0&\frac{2}{7}&\frac{1}{7}&0&0&0\\\hline
      1&0&0&0&0&\frac{1}{7}\\\hline
      2&0&\frac{1}{7}&\frac{1}{7}&\frac{1}{7}&0\\\hline
    \end{tabular}
  \end{center}
  The marginal PMF of $X$ is thus given by
  \[
    f_X(x)=
    \begin{cases}
      \frac{2}{7}&\text{for \(x=2,3\),}\\
      \frac{1}{7}&\text{for \(x=4,5,6\)}
    \end{cases}
  \]
  and the marginal PMF of $Y$ is given by
  \[
    f_Y(x)=
    \begin{cases}
      \frac{3}{7}&\text{for \(x=0,2\),} \\
      \frac{1}{7}&\text{for \(x=1\).}\\
    \end{cases}
  \]
  So the mean and variance of $X$ and $Y$ are
  \begin{align*}
    \mu_X
    &=\frac{4+6+4+5+6}{7}
    &\mu_Y
    &=1,\\
    &=\frac{25}{7},\\
    \Var(X)
    &=\frac{8+18+16+25+36}{7}-\left(\frac{25}{7}\right)&
    \Var(Y)&=\frac{1+12}{7}-1\\
    &=\frac{96}{49},
    &&=\frac{6}{7}.
  \end{align*}
  Lastly, the correlation between $X$ and $Y$ is
  \[
    \rho_{X,Y}
    =\frac{5}{\sqrt{\frac{576}{7}}}
    \approx\num{0.551198189805123}.\qedhere
  \]
\end{solution}

\begin{problem}[Handout 15, \# 11]
  Consider the joint PMF \(p(x,y)=cxy\), \(1\leq x\leq 3\), \(1\leq y\leq
  3\).
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Find the normalizing constant \(c\).
  \item Are \(X\) and \(Y\) independent? Prove your claim.
  \item Find the expectations of \(X\), \(Y\), and \(XY\).
  \end{enumerate}
\end{problem}
\begin{solution}
  \emph{Remark:} Note that below parts (a), (b), and (c) are out of order.
  \\\\
  For part (a): The normalizing constant is $c=\frac{1}{36}$; this is
  because
  \[
    \sum_{x,y =(1,1)}^{(3,3)} cxy = 36c
  \]

  For part (c): First,
  \[
    E(X) = E(Y) = \sum_{x=1}^3 x^2(1+2+3)c = 6c \sum_{x=1}^3
    x^2 = \frac{7}{3}
  \]
  and
  \[
    E(XY) = \sum_{(x,y) = (1,1)}^{(3,3)} cx^2y^2 =\frac{49}{9}
  \]

  For part (b): We see that $X$ and $Y$ are independent;
  $E(XY) = E(X)E(Y)$.
\end{solution}

\begin{problem}[Handout 15, \# 12]
  A fair die is rolled twice. Let \(X\) be the maximum and \(Y\) the
  minimum of the two rolls. By using the joint PMF of \(X\) and \(Y\)
  worked out in the text, find the PMF of \(\frac{X}{Y}\), and hence the
  mean of \(\frac{X}{Y}\).
\end{problem}
\begin{solution}
  The PMF of $\frac{X}{Y}$ is given by
  \[
    f_{\frac{X}{Y}}(x)=
    \begin{cases}
      \frac{1}{6}&\text{for \(x=1,2\),} \\
      \frac{1}{9}&\text{for \(x=\frac{3}{2},3\),} \\
      \frac{1}{18}&\text{for%
        \(x=\frac{5}{2},%
        4,5,6,\frac{5}{3},%
        \frac{4}{3},\frac{5}{4},%
        \frac{5}{6}\).}
    \end{cases}
  \]

  So that the mean is
  \[
    \mu_{\frac{X}{Y}} = \frac{487}{216} \approx\num{2.2546296296296298}\qedhere
  \]
\end{solution}

\begin{problem}[Handout 15, \# 13]
  Two random variables have the joint PMF \(p(x,x+1)=\frac{1}{n+1}\),
  \(x=0,\dotsc,n\). Answer the following question with as little
  calculation as possible.
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Are \(X\) and \(Y\) independent?
  \item What is the variance of \(Y-X\)?
  \item What is \(\Var(Y\,|\,{X=1})\)?
  \end{enumerate}
\end{problem}
\begin{solution}
  For part (a): No. The probability that $Y=2$ given that $X=1$ is $1$, but
  the probability that $Y=2$ is $\frac{1}{n+1}$.

  For part (b): $\Var(Y-X) = 0$, because $Y-X$ is constant; it is
  always $1$.

  For part (c): $\Var(Y\,|\,X=1) = 0$, because $Y = 2$ if $X=1$.
\end{solution}

\begin{problem}[Handout 15, \# 14]
  \emph{(Binomial Conditional Distribution).} Suppose \(X\) and \(Y\) are
  independent random variables, and \(X\sim\Bin(m,p)\),
  \(Y\sim\Bin(n,p)\). Show that the conditional distribution of \(X\) given
  by \(X+Y=t\) is a hypergeometric distribution; identify the parameters of
  this hypergeometric distribution.
\end{problem}
\begin{solution}
  First, let us find the PMF of \(X\) given \(X+Y=t\):
  \begin{align*}
    P(X=x\,|\,X+Y=t)
    &=\frac{P\bigl(\{\,X=x\,\}\cap\{\,X+Y=t\,\}\bigr)}{P(X+Y=t)}\\
    &=\frac{P(Y=t-x)}{P(X+Y=t)}\\
    &=\frac{\binom{n}{x}\binom{m}{t-x}p^t(1-p)^{m+n-t}}
      {\binom{m+n}{t}p^t(1-p)^{m+n-t}}\\
    &=\frac{\binom{n}{x}\binom{m}{t-x}}{\binom{m+n}{t}}.
  \end{align*}
  This distribution is precisely \(\Hypergeo(t,m,n+m)\).
\end{solution}

\begin{problem}[Handout 15, \# 15]
  Suppose a fair die is rolled twice. Let \(X\) and \(Y\) be the two
  rolls. Find the following with as little calculation as possible.
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item \(E(X+Y\,|\,Y=y)\).
  \item \(E(XY\,|\,Y=y)\).
  \item \(\Var(X^2Y\,|\,Y=y)\).
  \item \(\rho_{X+Y,X-Y}\).
  \end{enumerate}
\end{problem}
\begin{solution}
  For part (a):
  \[E(X+Y\,|\,Y=y) = E(X\,|\,Y=y) + E(Y\,|\,Y=y) = 3.5+y.\]

  For part (b):
  \[E(XY\,|\,Y=y) = E(X\,|\,Y=y)E(Y\,|\,Y=y) = 3.5y.\]

  For part (c):
  \[
    \Var(X^2Y\,|\,Y=y)=E((X^2Y)^2\,|\,Y=y)-E(X^2Y\,|\,Y=y)^2
    =c^2\left(\frac{91}{6}-3.5\right).
  \]

  For part (d):
  \begin{align*}
    \Cov(X+Y,X-Y)
    &=E((X+Y)(X-Y))-E(X+Y)E(X-Y)\\
    &=E(X)E(X)-E(Y)E(Y)-E(X)E(X)+E(Y)E(Y)\\
    &=0,
  \end{align*}
  so $\rho_{X+Y,X-Y} = 0$.
\end{solution}

\begin{problem}[Handout 15, \# 16]
  \emph{(A Standard Deviation Inequality).} Let \(X\) and \(Y\) be two
  random variables. Show that \(\sigma_{X+Y}\leq\sigma_X+\sigma_Y\).
\end{problem}
\begin{solution}
  Suppose \(\sigma_X\) and \(\sigma_Y\) exist and are finite. We want to
  show
  \[
    \sigma_{X+Y}\leq\sigma_X+\sigma_Y;
  \]
  this is the same as showing that
  \begin{align*}
    \sigma_{X+Y}^2
    &\leq\sigma_X+\sigma_Y^2+2\sigma_X\sigma_Y\\
    \Var(X+Y)&\leq\Var(X)+\Var(Y)+2[\Var(X)\Var(Y)]^{\frac{1}{2}}.
  \end{align*}

  First, let us expand \(\Var(X+Y)\) using the definition of variance, we
  have
  \begin{align*}
    \Var(X+Y)
    &=E\bigl((X+Y)^2\bigr)-E(X+Y)^2\\
    &=E(X^2)+2E(XY)+E(Y^2)-E(X)^2-2E(X)E(Y)-E(Y)^2\\
    &=\bigl(E(X^2)-E(X)^2\bigr)+\bigl(E(Y^2)-E(Y)^2\bigr)+2[E(XY)-E(X)E(Y)]\\
    &=\Var(X)+\Var(Y)+2[E(XY)-E(X)E(Y)].
  \end{align*}
  Therefore, it suffices to show that
  \[
    E(XY)-E(X)E(Y)\leq [\Var(X)\Var(Y)]^{\frac{1}{2}},
  \]
  or, rewritten using covariance,
  \[
    \Cov(X,Y)^2\leq\Var(X)\Var(Y).
  \]

  By the Cauchy--Schwartz inequality, we have
  \begin{align*}
    \Cov(X,Y)^2
    &=E\bigl[(X-E(X))(Y-E(Y))\bigr]^2\\
    &\leq E\bigl[(X-E(X))^2\bigr]
      E\bigl[(Y-E(Y))^2\bigr]\\
    &=\Var(X)\Var(Y).\qedhere
  \end{align*}
\end{solution}

\begin{problem}[Handout 15, \# 17]
  Seven balls are distributed randomly in seven cells. Let \(X_k\) be
  the number of cells containing exactly \(k\) balls. Using the
  probabilities tabulated in II, 5, write down the joint distribution of
  \(X_2,X_3\).
\end{problem}
\begin{solution}
  The tabled referenced in this problem is on p.\@ 40 of Feller. Let us
  write down a table of our own for the joint distribution of
  \((X_2,X_3)\):
  \begin{center}
    \begin{tabular}{L|L|L|L|L|}
      _{X_3}\backslash^{X_2}&0&1&2&3\\\hline
      0&\num{0.047539}&\num{0.156368}&\num{0.321295}&\num{0.107098}\\\hline
      1&\num{0.108883}&\num{0.214197}&\num{0.026775}&0\\\hline
      2&\num{0.017850}&0&0&0\\\hline
    \end{tabular}.
  \end{center}

  Let us do a sanity check by summing over all of the entries in the table
  above
  \[
    \num{0.047539}+\num{0.156368}+\num{0.321295}+\num{0.107098}+
    \num{0.108883}+\num{0.214197}+\num{0.026775}+0+\num{0.017850}+0+0+0\approx
    1.\qedhere
  \]
\end{solution}

\begin{problem}[Handout 15, \# 18]
  Two ideal dice are thrown. Let \(X\) be the score on the first die and
  \(Y\) be the larger of two scores.
  \begin{enumerate}[label=(\alph*),noitemsep]
  \item Write down the joint distribution of \(X\) and \(Y\).
  \item Find the means, the variances, and the covariance.
  \end{enumerate}
  \end{problem}
\begin{solution}
  For part (a): The random variable \(X\) takes on integer values between
  zero and six and so does \(Y\). Moreover, the dependence of \(Y\) on
  \(X\) tells us that \(P(\{\,X=k\,\}\cap\{\,Y=\ell\,\})=0\) if
  \(\ell<k\); this allows us to fill in a significant portion of the joint
  distribution table:
  \begin{center}
    \begin{tabular}{L|L|L|L|L|L|L|}
      _Y\backslash^X&1&2&3&4&5&6\\\hline
      1&\frac{1}{36}&0&0&0&0&0\\\hline
      2&\frac{1}{36}&\frac{2}{36}&0&0&0&0\\\hline
      3&\frac{1}{36}&\frac{1}{36}&\frac{3}{36}&0&0&0\\\hline
      4&\frac{1}{36}&\frac{1}{36}&\frac{1}{36}&\frac{4}{36}&0&0\\\hline
      5&\frac{1}{36}&\frac{1}{36}&\frac{1}{36}&\frac{1}{36}&\frac{5}{36}&0\\\hline
      6&\frac{1}{36}&\frac{1}{36}&\frac{1}{36}&\frac{1}{36}&\frac{1}{36}&\frac{6}{36}\\\hline
    \end{tabular}.
  \end{center}
  (One can easily verify that the sum of the entries in this table do in
  fact add up to one.)

  For part (b): We can recover the individual PMFs for \(X\) and \(Y\)
  using the table in part (a) and so recover the mean and variance. These
  are
  \begin{align*}
    E(X)&=\frac{6}{36}%
          +2\left(\frac{6}{36}\right)%
          +3\left(\frac{6}{36}\right)%
          +4\left(\frac{6}{36}\right)%
          +5\left(\frac{6}{36}\right)%
          +6\left(\frac{6}{36}\right)\\
        &=3.5,\\
    E(X)&=1^2\left(\frac{6}{36}\right)%
          +2^2\left(\frac{6}{36}\right)%
          +3^2\left(\frac{6}{36}\right)%
          +4^2\left(\frac{6}{36}\right)%
          +5^2\left(\frac{6}{36}\right)%
          +6^2\left(\frac{6}{36}\right)\\
        &\approx\num{15.166666666666666},\\
    \Var(X)&\approx\num{2.916666666666666},
  \end{align*}
  and
  \begin{align*}
    E(Y)&=\frac{1}{36}%
          +2\left(\frac{3}{36}\right)%
          +3\left(\frac{5}{36}\right)%
          +4\left(\frac{7}{36}\right)%
          +5\left(\frac{9}{36}\right)%
          +6\left(\frac{11}{36}\right)\\
        &\approx\num{4.472222222222222},\\
    E(Y^2)&=1^2\left(\frac{1}{36}\right)%
          +2^2\left(\frac{3}{36}\right)%
          +3^2\left(\frac{5}{36}\right)%
          +4^2\left(\frac{7}{36}\right)%
          +5^2\left(\frac{9}{36}\right)%
          +6^2\left(\frac{11}{36}\right)\\
        &\approx\num{21.97222222222222},\\
    \Var(Y)&\approx\num{1.9714506172839492},
  \end{align*}
  and lastly (after a long calculation which we omit) the covariance is
  \[
    \Cov(X,Y)\approx\num{2.0611111111111153}.\qedhere
  \]
\end{solution}

\begin{problem}[Handout 15, \# 19]
  Let \(X_1\) and \(X_2\) be independent and have the common geometric
  distribution \(\{q^kp\}\) (as in problem 4). Show without calculations
  that the \emph{conditional distribution of \(X_1\) given \(X_1+X_2\) is
    uniform}, that is,
  \begin{equation}
    \label{eq:12:uniform-conditional-pmf}
    P(X_1=k\,|\,X_1+X_2=n)=\frac{1}{n+1},\quad k=0,\dotsc,n.
  \end{equation}
\end{problem}
\begin{solution}
  By definition of conditional probability, we have
  \begin{align*}
    P(X_1=k\,|\,X_1+X_2=n)
    &=\frac{P(\{\,X_1=k\,\}\cap\{\,X_1+X_2=n\,\})}{P(X_1=k)}\\
    &=\frac{P(X_2=n-k)}{P(X_1+X_2=n)}\\
    &=\frac{q^{n-k}p}{q^{n-k}p(n+1)}\\
    &=\frac{1}{n+1}.\qedhere
  \end{align*}
\end{solution}

\begin{problem}[Handout 15, \# 20]
  If two random variables \(X\) and \(Y\) assume only two values
  each, and if \(\Cov(X,Y)=0\), then \(X\) and \(Y\) are
  independent.
\end{problem}
\begin{solution}
  We show that the joint PDF of \((X,Y)\) is
  \[
    f_{X,Y}(x,y)=f_X(x)f_Y(y).
  \]

  Suppose \(X\) assumes the values \(\{a,b\}\) and \(Y\) assumes the values
  \(\{c,d\}\) where, without loss of generality, we may assume \(a<b\) and
  \(c<d\); however, we may have \(a=c\), \(b=c\), \(a=d\), etc. Let
  \(p_a\), \(p_b\), \(p_c\), and \(p_d\) be the probabilities associated to
  \(a\), \(b\), \(c\), and \(d\), respectively. Then, we have
  \[
    \begin{aligned}
      p_a+p_b&=1,&p_c+p_d&=1,
    \end{aligned}
  \]
  and more significantly
  \begin{align*}
    \Cov(X,Y)
    &=E(XY)-E(X)E(Y)\\
    E(XY)&=(ap_a+bp_b)(cp_c+dp_d)\\
    \sum_{\substack{x\in\{a,b\},\\ y\in\{c,d\}}}xy f_{X,Y}(x,y)
    &=(ap_a+bp_b)(cp_c+dp_d)\\
    \begin{aligned}
      ac&f_{X,Y}(a,c)+adf_{X,Y}(a,d)\\&+bcf_{X,Y}(b,c)+bdf_{X,Y}(b,d)
    \end{aligned}
    &=
      \begin{aligned}
        ac&p_ap_c+adp_ap_d\\&+bcp_bp_c+bdp_bp_d.
      \end{aligned}
  \end{align*}
  A term by term comparison shows that we must have
  \[
    f(x,y)=xyp_xp_y
  \]
  for \(x\in\{a,b\}\), \(y\in\{c,d\}\). Thus,
  \(f_{X,Y}(x,y)=f_X(x)f_Y(y)\); i.e., \(X\) and \(Y\) are independent.
\end{solution}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../MA519-HW-ALL"
%%% End:
