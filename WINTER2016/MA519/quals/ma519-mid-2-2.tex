\subsection{Midterm 2}
This is actually the past final.
\begin{problem}
  Consider the infinitely many independent, identical experiments of
  throwing a pair of dice and the outcome of each experiment is the sum of
  the two numbers. Let \(N\geq 1\) be the number of experiments such that
  the number \(5\) or \(7\) first appears as the outcome. Let further \(X\)
  be that number at the \(N\)\textsup{th} experiment; i.e., \(X\) can be
  either a \(5\) or a \(7\).
  \begin{alphlist}
  \item Find \(P(N=n)\) for \(n\geq 1\).
  \item Find \(P(X=5)\).
  \item Prove or disprove that \(N\) and \(X\) are independent.
  \end{alphlist}
\end{problem}
\begin{solution*}
  For part (a): It is clear that \(N\sim\Geo(p)\) where \(p\) here is the
  probability of landing a \(5\) or a \(7\). Let us explicitly compute
  \(p\).

  Assuming each event is equally likely, there are \(4\) ways to make a
  \(5\) (rolling a \(4\)-\(1\) or \(2\)-\(3\) and vice-versa) and \(6\)
  ways to make a \(7\) (rolling a \(6\)-\(1\), \(5\)-\(2\), or \(4\)-\(3\)
  and vice-versa). Therefore,
  \(p=\frac{4}{36}+\frac{6}{36}=\frac{5}{18}\).

  Consequently the PMF of \(N\) is given by
  \[
    P(N=n)=\left(1-\tfrac{5}{18}\right)^{n-1}\tfrac{5}{18}
    =\left(\tfrac{13}{18}\right)^{n-1}\tfrac{5}{18}.
  \]
  \\\\
  For part (b): As we will soon show, \(X\) is independent from \(N\) so we
  should be able to compute \(P(X=5)\) without working with \(N\) directly
  (unlike the solution proposed by Prof.\@ Yip). Indeed, since each
  experiment is independent the probability of \(\{\,X=5\,\}\) comes down
  to a computation of conditional probabilities like we now show
  \[
    P(X=5)=\frac{P(\text{the experiment returns a \(5\)})}
    {P(\text{the experiment returns either a \(5\) or a \(7\)})}
    =\frac{4/36}{10/36}=\frac{2}{5}.
  \]
  \\\\
  For part (c): To show that \(N\) and \(X\) are independent we must show
  that
  \[
    P(N=n,X=x)=P(N=n)P(X=x).
  \]
  Expanding \(P(N=n,X=x)\) a little we see that
  \[
    P(N=n,X=x)=P(N=n\mid X=x)P(X=x)
  \]
  so it suffices to show that \(P(N=n\mid X=x)=P(N=n)\); but this is
  clear. Therefore, \(X\) and \(N\) are independent.
\end{solution*}

\begin{problem}
  Consider a city in which the male and female drivers occupy \(\alpha\)
  and \(1-\alpha\) fractions of the whole city driver population.  In any
  given year, a male and female driver will have an accident with
  probability \(p_M\) and \(p_F\). Assume that the behavior of each driver
  is independent from year to year.

  Now a driver is randomly chosen. Let \(A_k\) be the event that this
  driver will have an accident in the \(k\)\textsup{th} year. Let \(M\) be
  the event that the randomly chosen driver is male.
  \begin{alphlist}
  \item Suppose \(p_M>p_F\). Show that \(P(M\mid A_k)>P(M)\).
  \item Suppose \(p_M\neq p_F\). Show that \(P(A_2\mid A_1)>P(A_1)\).
  \end{alphlist}
\end{problem}
\begin{solution*}
  For part (a): Let us first explicitly compute some of the probabilities
  stated in the problem for use later in its solution. First off,
  \(P(M)=\alpha\); i.e., the ratio of male drivers. Now, we can compute
  \(P(M\mid A_k)\) using the definition of conditional probability and the
  total probability formula as we now show
  \begin{equation}
    \label{eq:yip:mid-2:2-a}
    P(M\mid A_k)=\frac{P(M\cap A_k)}{P(A_k)}=\frac{P(M\cap
      A_k)}{P(A_k\mid M)P(M)+P(A_k\mid F)P(F)}.
  \end{equation}
  We know some of the values in \eqref{eq:yip:mid-2:2-a} explicitly so let
  us fill them in here
  \[
    P(M\mid A_k)=\frac{p_M}{\alpha p_M+(1-\alpha)p_F}.
  \]

  At last we have collected the necessary data to show that
  \begin{align*}
    P(M\mid A_k)-P(M)
    &=\frac{p_M}{p_M\alpha+(1-\alpha)p_F}-\alpha\\
    &=\frac{p_M-\alpha^2 p_M+(1-\alpha)\alpha p_F}{\alpha
      p_M+(1-\alpha)p_F}\\
    &=\frac{(1-\alpha^2)p_M-(1-\alpha)\alpha p_F}{\alpha p_M+(1-\alpha)p_F}\\
    &=\frac{(1-\alpha)(1+\alpha)p_M-(1-\alpha)\alpha p_F}{\alpha
      p_M+(1-\alpha)p_F}\\
    &=(1-\alpha)\frac{(1+\alpha)p_M-\alpha p_F}{\alpha p_M+(1-\alpha)p_F}\\
    &>0
  \end{align*}
  since \(p_M>p_F\) and \(1+\alpha>\alpha\); i.e., \(P(M\mid A_k)>P(M)\).
  \\\\
  For part (b): Similar to our analysis above if we unravel the definition
  of conditional probability we arrive at the following equality
  \begin{align*}
    P(A_2\mid A_1)
    &=\frac{P(A_2\cap A_1)}{P(A_1)}\\
    &=\frac{P(A_2\cap A_1\mid M)P(M)+P(A_2\cap A_1\mid F)P(F)}
      {P(A_1\mid M)P(M)+P(A_1\mid F)P(F)}\\
    &=\frac{\alpha p_M^2+(1-\alpha)p_F^2}{\alpha p_M+(1-\alpha)p_F}.
  \end{align*}
  Therefore,
  \begin{align*}
    P(A_2\mid A_1)-P(A_1)
    &=\frac{\alpha p_M^2+(1-\alpha)p_F^2}{\alpha p_M+(1-\alpha)p_F}
    -\alpha p_M-(1-\alpha)p_F\\
    &=\frac{\alpha p_M^2+(1-\alpha)p_F^2
      -\alpha^2p_M^2-2\alpha(1-\alpha)p_Mp_F
      -(1-\alpha)^2p_F^2}{\alpha p_M+(1-\alpha)p_F}\\
    &=\frac{(\alpha-\alpha^2)p_M^2+((1-\alpha)-(1-\alpha)^2)p_F^2-2\alpha(1-\alpha)p_Mp_F}
      {\alpha p_M+(1-\alpha)p_F}\\
    &=\frac{(\alpha-\alpha^2)p_M^2+(\alpha-\alpha^2)p_F^2-2\alpha(1-\alpha)p_Mp_F}
      {\alpha p_M+(1-\alpha)p_F}\\
    &=\alpha(1-\alpha)\frac{p_M^2+p_F^2-2p_Mp_F}{\alpha
      p_M+(1-\alpha)p_F}\\
    &=\alpha(1-\alpha)\frac{(p_M-p_F)^2}{\alpha
      p_M+(1-\alpha)p_F}\\
    &>0
  \end{align*}
  which holds since \(p_M\neq p_F\). Thus,
  \[
    P(A_2\mid A_1)>P(A_1)
  \]
  as we wanted to show.
\end{solution*}

\begin{problem}
  Let \(X_1,\dotsc,X_n\) be a collection of IID exponential random
  variables with parameter \(\lambda\). Let
  \begin{align*}
    Y_1&=X_1,\\
    Y_2&=X_1+X_2,\\
       &\vdotswithin{{}={}}\\
    Y_n&=X_1+\dotsb+X_n.
  \end{align*}
  Find the joint PDF \(p(y_1,\dotsc,y_n)\) of \((Y_1,\dotsc,Y_n)\).
\end{problem}
\begin{solution*}
  In DasGupta's class we defined the PDF of an exponential random variable
  \(X\) to be
  \[
    P(X=x)=
    \begin{cases}
      \tfrac{1}{\lambda}\rme^{-\frac{1}{\lambda}x}&\text{for \(x>0\),}\\
      0&\text{otherwise}
    \end{cases}
  \]
  and this shall be the definition we use here.

  Now, let us now compute the joint PDF of \((Y_1,\dotsc,Y_n)\)
  \begin{equation}
    \label{eq:yip:mid-2:3}
    \begin{aligned}
      p(y_1,\dotsc,y_n)
      &=P(Y_n=y_n,\dotsc,Y_1=y_1)\\
      &=P(Y_1=y_1,\dotsc,Y_{n-1}=y_{n-1})\\
      &\phantom{{}={}\dotsm}
      P(Y_n=y_n\mid Y_1=y_2,\dotsc,Y_{n-1}=y_{n-1})\\
      &=P(Y_1=y_1)P(Y_2=y_2\mid Y_1=y_1)\\
      &\phantom{{}={}}\dotsm%
      P(Y_2=y_2\mid Y_3=y_3,\dotsc,Y_n=y_n)\\
      &\phantom{{}={}\dotsm\dotsm}P(Y_n=y_n\mid Y_1=y_1,\dotsc,Y_{n-1}=y_{n-1}).
    \end{aligned}
  \end{equation}

  Now, if we examine the product \(P(Y_n=y_n)P(Y_{n-1}=y_{n-1}\mid
  Y_n=y_n)\) we see that
  \[
    \begin{aligned}
      P(Y_1=y_1)P(Y_2=y_2\mid Y_1=y_1)&=%
      \begin{cases}
        \bigl(\frac{1}{\lambda}\rme^{-\frac{1}{\lambda}y_1}\bigr)%
        \bigl(\frac{1}{\lambda}\rme^{-\frac{1}{\lambda}(y_2-y_1)}\bigr)
        &\text{for \(y_1,y_2>0\),}\\
        0&\text{otherwise}
      \end{cases}\\
      &=\begin{cases}
        \frac{1}{\lambda^2}\rme^{-\frac{1}{\lambda}y_2}
        &\text{for \(y_1,y_2>0\),}\\
        0&\text{otherwise}
    \end{cases}
  \end{aligned}
  \]
  and so forth for \(1\leq k\leq n\). This same analysis applied to
  \eqref{eq:yip:mid-2:3} gives us
  \[
    p(y_1,\dotsc,y_n)=
    \begin{cases}
      \frac{1}{\lambda^n}\rme^{-\frac{1}{\lambda}y_n}&\text{for \(y_n>0\),}\\
      0&\text{otherwise.}
    \end{cases}\qedhere
  \]
\end{solution*}

\begin{problem}
  The PDF \(p(x)\) of the gamma distribution with parameter (\(\alpha>0\),
  \(\lambda>0\)) is given by
  \[
    p(x)=
    \begin{cases}
      \frac{\lambda}{\Gamma(\alpha)}\rme^{-\lambda x}(\lambda x)^{\alpha
        -1}
      &\text{for \(x\geq 0\),}\\
      0&\text{otherwise.}
    \end{cases}
  \]
  Let \(X\) and \(Y\) be independent gamma distributed random variables
  with parameters \((\alpha,\lambda)\) and \((\beta,\lambda)\). Show
  analitically that \(X+Y\) has a gamma distribution with parameter
  \((\alpha+\beta,\lambda)\).

  Show how as a byproduct that the above conclusion leads to the following
  integration identity for \(\alpha,\beta>0\)
  \[
    \int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\diff x
    =\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}.
  \]
\end{problem}
\begin{solution*}
  We show that \(X+Y\sim G(\alpha+\beta,\lambda)\) by showing that the MGF
  of \(X+Y\) is that of a gamma. But first, let us find the MGF of a gamma
  \begin{lemma}
    Let \(X\sim G(\alpha,\lambda)\). Then
    \[
      E(\rme^{tX})=
    \]
  \end{lemma}
  \begin{solution*}[Proof]
    \renewcommand\qedsymbol{\(\scriptstyle\clubsuit\)}
    A straightforward calculation gives us the desired equality
    \begin{align*}
      E(\rme^{tX})
      &=\int_0^\infty\sum_{k=0}^\infty \frac{t^k}{k!}
        \bigl(\tfrac{\lambda}{\Gamma(\alpha)}
        \rme^{-\lambda x}(\lambda x)^{\alpha-1}\bigr)^k\diff x\\
      &=
    \end{align*}
  \end{solution*}
\end{solution*}

\begin{problem}
  Let \(X_1,\dotsc,X_n\) be a collection of IID random variables with
  expectations and variances equal to \(\mu\) and \(\sigma^2\). Define the
  \emph{sample mean} \(\bar X\) and \emph{sample variance} \(S^2\) as
  \[
    \bar X=\tfrac{1}{n}(X_1+\dotsb+X_n),\quad
    S^2=\frac{1}{n}\sum_{k=1}^n(X_k-\bar X)^2.
  \]
  Compute \(\Var(\bar X)\) and \(E(S^2)\).
\end{problem}
\begin{solution*}
\end{solution*}

\begin{problem}[Estimation of the length of an interval]
  Let \(l>0\) be some unknown but fixed length. Let \(X_1,X_2,\dotsc\), be
  a sequence of IID random variables uniformly distributed on
  \([0,l]\). The goal is to use the \(X_k\) to estimate \(l\).
  \begin{alphlist}
  \item Let \(A_n=\tfrac{2}{n}(X_1+\dotsb+X_n).\) Show that \(A_n\) is an
    unbiased estimation in the sense that \(E(A_n)=l\).
  \item Let \(B_n=\gamma_n\max\{X_1,\dotsc,X_n\}\) where \(\gamma_n\) is
    some number. Find the correct value of \(\gamma_n\) such that \(B_n\)
    is also an unbiased estimator.
  \item Find \(\Var(A_n)\) and \(\Var(B_n)\).
  \item Which
  \end{alphlist}
\end{problem}
\begin{solution*}
\end{solution*}

%%% Local Variables:
%%% TeX-master: "../MA519-HW-ALL"
%%% End:
