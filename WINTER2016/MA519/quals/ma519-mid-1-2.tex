\section{Midterms and Finals (Yip)}
\subsection{Midterm 1}
The author of this document apologizes for any ungrammatical content in
this document. We present the problems as they are given to us and only
correct grammar and orthography when it is obvious what the author meant.
\begin{problem}
  Suppose \(n\) balls are distributed at random into \(r\) boxes in such a
  way that each ball chooses a box independently of each other. Let \(S\)
  be the number of \emph{empty boxes}. Compute \(E(S)\) and \(\Var(S)\).

  \noindent\emph{Hint:} Consider the random variables \(X_k\), for
  \(k=1,\dotsc,r\), which equals \(1\) if the \(k\)\textsup{th} box is
  empty and \(0\) otherwise. Relate \(S\) and the \(X_k\).
\end{problem}
\begin{solution*}
  Write \(S=\sum_{k=1}^n X_k\) where
  \[
    X_k
    =
    \begin{cases}
      1&\text{if the \(k\)\textsup{th} box is empty,}\\
      0&\text{otherwise.}
    \end{cases}
  \]
  Then, by the definition of expected value, we have
  \begin{align*}
    E(S)
    &=E\left(\sum\nolimits_{k=1}^r X_k\right)\\
    &=\sum_{k=1}^n E(X_k)\\
    \shortintertext{which, by the tail-sum formula, becomes}
    &=rE(X_1).
  \end{align*}
  Hence, to compute \(E(S)\) we need first compute \(E(X_1)\), as we now
  do. Unraveling the definition of expected value for \(X_1\) we have
  \[
    E(X_1)=0\cdot P(X_1=0)+1\cdot P(X_1=1)=P(X_1=1)
  \]
  where
  \[
    P(X_1=1)=\frac{(r-1)^n}{r^n}.
  \]
  Thus,
  \[
    E(S)=\frac{r(r-1)^n}{r^n}=\frac{(r-1)^n}{r^{n-1}}.
  \]

  As for the variance, the second moment can likewise be computed as
  \begin{align*}
    E(S^2)
    &=E\left[\left(\sum\nolimits_{k=1}^r X_k\right)^2\right]\\
    &=E\left(\sum\nolimits_{k=1}^r X_k^2\right)
      +E\left(\sum\nolimits_{j\neq k}^r X_jX_k\right)\\
    &=\sum_{k=1}^r E(X_k^2)+\sum_{j\neq k} E(X_jX_k)\\
    &=\sum_{k=1}^r E(X_k)+\sum_{j\neq k}E(X_jX_k)\\
    &=\frac{(r-1)^n}{r^{n-1}}+\sum_{j\neq k}E(X_jX_k).
  \end{align*}
  Now, the sum
  \[
    \sum_{j\neq k}E(X_jX_k)=%
    \sum_{j\neq k}P(X_j=1,X_k=1)=\sum_{j\neq
      k}\left(1-\tfrac{2}{r}\right)^n=%
    \binom{r}{2}\frac{(r-2)^n}{r^n}.
  \]
  Thus,
  \[
    \Var(S)=%
    \frac{(r-1)^n}{r^{n-1}}+%
    \binom{r}{2}\frac{(r-2)^n}{r^n}-%
    \frac{(r-1)^{2n}}{r^{2n-2}}.\qedhere
  \]
\end{solution*}

\begin{problem}
  Suppose \(n\) balls are distributed in \(n\) boxes in such a way that
  each ball chooses a box independently of each other.
  \begin{alphlist}
  \item What is the probability that box \# 1 is empty?
  \item What is the probability that only box \# 1 is empty?
  \item What is the probability that only one box is empty?
  \item Given that box \# 1 is empty, what is the probability that only one
    box is empty?
  \item Given that only one box is empty, what is the probability that box
    \# 1 is empty?
  \end{alphlist}

  \noindent\emph{Hint:} make use of the fact that the number of balls and
  boxes are the same.
\end{problem}
\begin{solution*}
  For part (a), the probability that box \# 1 is empty can be quickly shown
  to be
  \begin{equation}
    \label{eq:yip:mid-1:2-a}
    \frac{(n-1)^n}{n^n};
  \end{equation}
  i.e., there are \(n^n\) ways to distribute the \(n\) balls among the
  \(n\) boxes; then there are \((n-1)^n\) ways to distribute the \(n\)
  balls among the \(n-1\) remaining boxes.
  \\\\
  For part (b), the probability that only box \# 1 is empty is
  \begin{equation}
    \label{eq:yip:mid-1:2-b}
    \frac{\binom{n}{2}(n-1)!}{n^n};
  \end{equation}
  i.e., each box must contain at least \(1\) ball and there are
  \(\binom{n}{2}(n-1)!\) ways to distribute \(2\) of the \(n\) balls among
  the remaining \(n-1\) boxes.
  \\\\
  For part (c), using \eqref{eq:yip:mid-1:2-b}, the probability that
  exactly one box is empty is
  \[
    \frac{n\binom{n}{2}(n-1)!}{n^n};
  \]
  i.e., there are \(\binom{n}{1}=n\) ways to choose a box to be empty once
  this is done, the probability that said box is empty is the same as the
  probability that box \# 1 is empty; and that probability we found in part
  (b).
  \\\\
  For part (d), by the definition of conditional probability together with
  parts \eqref{eq:yip:mid-1:2-a} and \eqref{eq:yip:mid-1:2-b} we have
  \[
    \frac{\binom{n}{2}(n-1)!/n^n}{(n-1)^n/n^n}=\frac{\binom{n}{2}(n-1)!}{(n-1)^n}.
  \]
  \\\\
  For part (e), the probability is clearly
  \[
    \frac{1}{n}
  \]
  since at most one box is empty and the event that box \# 1 is empty is
  one sample point in our conditioned probability space.
\end{solution*}

\begin{problem}
  McDonald's newest promotion is putting a toy inside every one of its
  hamburgers. Suppose there are \(N\) distinct types of toys and each of
  them is equally likely to be put inside any of the hamburges. What is the
  expected value and variance of the number of hamburgers you need to order
  (or eat) before you have a complete set of the \(N\) toys.

  \noindent\emph{Hint:} consider the number of hamburgers you need to order
  (or eat) in between getting one and two distinct types of toys, two and
  three distinct types of toys, and so forth.
\end{problem}
\begin{solution*}
  Setting the utter wackiness of this problem aside for a moment, let
  \(X_k\) denote the number of burgers we need to purchase in order to get
  the \(k\)\textsup{th} toy, for \(1\leq k\leq N\). Before preceding, note
  that each \(X_k\), for \(k>1\), is a \(\Geo(\frac{N-k+1}{N})\) random
  variable; i.e., in our \(j\)\textsup{th} try we succeed with in getting a
  new toy with probability \((\frac{N-k+1}{N})^j\). Then the total number
  of burgers we must purchase is
  \[
    X=\sum_{k=1}^n X_k.
  \]
  Thus,
  \begin{align*}
    E(X)
    &=\sum_{k=1}^n E(X_k)\\
    &=1+\frac{N}{N-1}+\dotsb+\frac{N}{1}\\
    &=N\left(\frac{1}{N}+\frac{1}{N-1}+\dotsb+1\right).
  \end{align*}
  For the variance, we have
  \begin{align*}
    \Var(X)
    &=\sum_{k=1}^n\Var(X_k)\\
    &=0+\frac{1/N}{((N-1)/N)^2}+\dotsb+\frac{(N-1)/N}{(1/N)^2}\\
    &=N\left(\frac{1}{(N-1)^2}+\frac{2}{(N-2)^2}+\dotsb+N-1\right).\qedhere
  \end{align*}
\end{solution*}

\begin{problem}
  Let \(X\) and \(Y\) be two independent geometric random variables with
  parameter \(p\).
  \begin{alphlist}
  \item Find the probability distribution function of \(\min\{X,Y\}\).
  \item Find the probability distribution function of \(\max\{X,Y\}\).
  \item Find the probability distribution function of \(X+Y\).
  \item Find \(P(X=j\mid X+Y=k)\) for \(j=1,\dotsc,k-1\).
  \end{alphlist}
\end{problem}
\begin{solution*}
  For part (a), let \(Z=\min\{X,Y\}\). Then using the axioms of
  probability, we can manipulate the expression for the PMF of \(Z\) into
  the following
  \begin{equation}
    \label{eq:yip:mid-1:4-a}
    \begin{aligned}
      P(Z=k)\\
      &=P(X=Y=k)+P(X=k,Y>k)+P(X>k,Y=k)\\
      &=P(X=k)P(Y=k)+P(X=k)P(Y>k)+P(X>k)P(Y=k).
    \end{aligned}
  \end{equation}
  Now let us find a closed form for the expression above. To make the
  analysis more digestible, we consider each term in the sum
  \eqref{eq:yip:mid-1:4-a} individually.

  First, we have
  \begin{align*}
    P(X=k)P(Y=k)
    &=(q^{k-1}p)(q^{k-1}p)\\
    &=q^{2k-2}p^2
  \end{align*}
  Next,
  \begin{align*}
    P(X=k)P(Y>k)
    &=q^{k-1}p\sum_{j\geq k}^\infty q^{j-1}p\\
    &=\sum_{j>k}^\infty (q^{2k-1}p^2)q^{j-k-1}\\
    &=q^{2k-1}p.
  \end{align*}
  Lastly, because \(X\) and \(Y\) are identically distributed, by symmetry
  \[
    P(X>k)P(Y=k)=q^{2k-1}p.
  \]
  Therefore, the PMF of \(Z\) is given by the expression
  \[
    P(Z=k)=q^{2k-2}p^2+2q^{2k-1}p.
  \]
  \\\\
  For part (b), let \(W=\max\{X,Y\}\). Then we can expand the PMF of \(W\)
  to get a simpler expression as follows
  \begin{equation}
    \label{eq:yip:mid-1:4-b}
    \begin{aligned}
      P(W=k)
      &=P(X=Y=k)+P(X=k,Y<k)+P(X<k,Y=k)\\
      &=P(X=k)P(Y=k)+2P(X=k)P(Y<k).
    \end{aligned}
  \end{equation}

  One term of the expression \eqref{eq:yip:mid-1:4-b} we already know from
  part (a); namely,
  \[
    P(X=k)P(Y=k)=q^{2k-2}p^2.
  \]
  As for the other term, we have
  \begin{align*}
    P(X=k)P(Y<k)
    &=q^{k-1}p\sum_{j=1}^k q^{j-1}p\\
    &=q^{k-1}\frac{1-q^k}{p}p^2\\
    &=q^{k-1}(1-q^k)p.
  \end{align*}

  Thus, the PMF of \(W\) is given by the expression
  \[
    P(W=k)=q^{2k-2}p^2+2q^{k-1}(1-q^k)p.
  \]
  \\\\\
  For part (c), since \(X\) and \(Y\) are independent the PMF of \(X+Y\) is
  given by the convolution
  \[
    P(X+Y=k)=\sum_{j=1}^{k-1} P(X=k-j)P(Y=j).
  \]
  Let us find a closed form for this expression,
  \begin{align*}
    P(X+Y=k)
    &=\sum_{j=1}^{k-1} P(X=k-j) P(Y=j)\\
    &=\sum_{j=1}^{k-1} (q^{(k-j)-1}p) (q^{j-1}p)\\
    &=\sum_{j=1}^{k-1}q^{k-2}p^2\\
    &=(k-1)q^{k-2}p^2.
  \end{align*}
  \\\\
  For part (d), by the definition of conditional probability, we have
  \begin{equation}
    \label{eq:yip:mid-1:4-d}
    P(X=j\mid X+Y=k)=%
    \frac{P(X=j,X+Y=k)}{P(X+Y=k)}=%
    \frac{P(X=j)P(Y=k-j)}{P(X+Y=k)}.
  \end{equation}
  By our answer to part (c) together with the PMFs of \(X\) and \(Y\),
  \eqref{eq:yip:mid-1:4-d} becomes
  \begin{align*}
    P(X=j\mid X+Y=k)
    &=\frac{(q^{j-1}p)(q^{k-j-1}p)}{(k-1)q^{k-2}p^2}\\
    &=\frac{1}{k-1}.\qedhere
  \end{align*}
\end{solution*}

\begin{problem}
  Suppose the events \(E\), \(F\), \(G\) are independent, in other words
  \begin{align*}
    P(E\cap F)&=P(E)P(F),\\
    P(F\cap G)&=P(F)P(G),\\
    P(G\cap E)&=P(G)P(E),\\
    P(E\cap F\cap G)&=P(E)P(F)P(G).
  \end{align*}
  Using he above definition, show that the following events are
  independent:
  \begin{alphlist}
  \item \(E\) and \(F^\rmc\);
  \item \(E\) and \(F\cap G^\rmc\);
  \item \(E\) and \(F^\rmc\cap G^\rmc\);
  \item \(E\) and \(F\cup G\);
  \item \(E\) and \(F\cup G^\rmc\).
  \end{alphlist}
\end{problem}
\begin{solution*}
  For part (a), we have
  \begin{align*}
    P(E\cap F^\rmc)
    &=P(E)-P(E\cap F)\\
    &=P(E)-P(E)P(F)\\
    &=P(E)(1-P(F))\\
    &=P(E)P(F^\rmc).
  \end{align*}
  \\\\
  For part (b), we have
  \begin{align*}
    P(E\cap (F\cap G^\rmc))
    &=P((E\cap F)\cap(E\cap G^\rmc))\\
    &=P(E\cap F)-P(E\cap G)\\
    &=P(E)P(F)-P(E)P(G)\\
    &=P(E)(P(F)-P(G))\\
    &=P(E)P(F\cap G^\rmc).
  \end{align*}
  \\\\
  For part (c), we have
  \begin{align*}
    P(E\cap F^\rmc\cap G^\rmc)
    &=P((E\cap F^\rmc)\cap(E\cap G^\rmc))\\
    &=P(E\cap F^{\rmc})-P(E\cap G)\\
    &=P(E)P(F^\rmc)-P(E)P(G)\\
    &=P(E)(P(F^\rmc)-P(G))\\
    &=P(E)(P(F^\rmc\cap G^\rmc)).
  \end{align*}
  \\\\
  For part (d), we have
  \begin{align*}
    P(E\cap F\cup G)
    &=P((E\cap F)\cup (E\cap G))\\
    &=P(E\cap F)+P(E\cap G)-P(E\cap F\cap G)\\
    &=P(E)P(F)+P(E)P(G)-P(E)P(F)P(G)\\
    &=P(E)(P(F)+P(G)-P(F)P(G))\\
    &=P(E)(P(F)+P(G)-P(F\cap G))\\
    &=P(E)P(F\cup G).
  \end{align*}
  \\\\
  For part (e), we have
  \begin{align*}
    P(E\cap F\cup G^\rmc)
    &=P((E\cap F)\cup(E\cap G^\rmc))\\
    &=P(E\cap F)+P(E\cap G^\rmc)-P(E\cap F\cap G^\rmc)\\
    &=P(E)P(F)+P(E)P(G^\rmc)-P(E)P(F)P(G^\rmc)\\
    &=P(E)(P(F)+P(G^\rmc)-P(F)P(G^\rmc))\\
    &=P(E)(P(F)+P(G^\rmc)-P(F\cap G^\rmc))\\
    &=P(E)P(F\cup G^\rmc).\qedhere
  \end{align*}
\end{solution*}

%%% Local Variables:
%%% TeX-master: "../MA519-HW-ALL"
%%% End:
